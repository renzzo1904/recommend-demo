{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE where original DATA is analyzed and Transformed \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is an extract of what we might find in our Google Analytics Data Stored in our Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Analysis of the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"google data/data.csv\")\n",
    "clicks_df = df.loc[df.Click == True]\n",
    "buy_df = pd.read_csv(\"google data/purchases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>date</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>total_hits</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>total_timeOnSite</th>\n",
       "      <th>total_bounces</th>\n",
       "      <th>total_newVisits</th>\n",
       "      <th>...</th>\n",
       "      <th>Category</th>\n",
       "      <th>Variant</th>\n",
       "      <th>ProductList</th>\n",
       "      <th>transactionId</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>itemTransactionId</th>\n",
       "      <th>Click</th>\n",
       "      <th>TotalPaid</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1470857552</td>\n",
       "      <td>1470857552</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Limited Supply/Bags/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1470862136</td>\n",
       "      <td>1470862136</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>341.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Limited Supply/Bags/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.79</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1470848713</td>\n",
       "      <td>1470848713</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Men's/Men's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Office/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Office/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   visitNumber     visitId  visitStartTime      date  total_visits  \\\n",
       "0            2  1470857552      1470857552  20160810             1   \n",
       "1            1  1470862136      1470862136  20160810             1   \n",
       "2            1  1470848713      1470848713  20160810             1   \n",
       "3            4  1470815734      1470815734  20160810             1   \n",
       "4            4  1470815734      1470815734  20160810             1   \n",
       "\n",
       "   total_hits  total_pageviews  total_timeOnSite  total_bounces  \\\n",
       "0           5                4             125.0            NaN   \n",
       "1           5                4             341.0            NaN   \n",
       "2           6                5             106.0            NaN   \n",
       "3           6                3               8.0            NaN   \n",
       "4           6                3               8.0            NaN   \n",
       "\n",
       "   total_newVisits  ...                            Category    Variant  \\\n",
       "0              NaN  ...           Home/Limited Supply/Bags/  (not set)   \n",
       "1              1.0  ...           Home/Limited Supply/Bags/  (not set)   \n",
       "2              1.0  ...  Home/Apparel/Men's/Men's-T-Shirts/  (not set)   \n",
       "3              NaN  ...                        Home/Office/  (not set)   \n",
       "4              NaN  ...                        Home/Office/  (not set)   \n",
       "\n",
       "   ProductList  transactionId Revenue itemTransactionId Click TotalPaid  \\\n",
       "0     Category            NaN     NaN               NaN  True       0.0   \n",
       "1     Category            NaN     NaN               NaN  True       0.0   \n",
       "2     Category            NaN     NaN               NaN  True       0.0   \n",
       "3     Category            NaN     NaN               NaN  True       0.0   \n",
       "4     Category            NaN     NaN               NaN  True       0.0   \n",
       "\n",
       "   Price Quantity  \n",
       "0  99.99      NaN  \n",
       "1   8.79      NaN  \n",
       "2  16.99      NaN  \n",
       "3   0.99      NaN  \n",
       "4   0.99      NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df.head()  # df where is located user liking behavior linked to clicked products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>date</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>total_hits</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>total_timeOnSite</th>\n",
       "      <th>total_bounces</th>\n",
       "      <th>total_newVisits</th>\n",
       "      <th>...</th>\n",
       "      <th>itemTransactionId</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>ProductSKU</th>\n",
       "      <th>Category</th>\n",
       "      <th>Variant</th>\n",
       "      <th>ProductList</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>TotalCost</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1472571985</td>\n",
       "      <td>1472571985</td>\n",
       "      <td>20160830</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ORD201608301467</td>\n",
       "      <td>24 oz YouTube Sergeant Stripe Bottle</td>\n",
       "      <td>GGOEYDHJ019399</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Single Option Only</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>20.91</td>\n",
       "      <td>7.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1472606991</td>\n",
       "      <td>1472606991</td>\n",
       "      <td>20160830</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>ORD201608301495</td>\n",
       "      <td>Google Laptop and Cell Phone Stickers</td>\n",
       "      <td>GGOEGFKQ020399</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Single Option Only</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>112.00</td>\n",
       "      <td>99.50</td>\n",
       "      <td>1.99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1472599227</td>\n",
       "      <td>1472599227</td>\n",
       "      <td>20160830</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ORD201608301510</td>\n",
       "      <td>20 oz Stainless Steel Insulated Tumbler</td>\n",
       "      <td>GGOEGDHQ014899</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Single Option Only</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>11.24</td>\n",
       "      <td>19.99</td>\n",
       "      <td>19.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1472594459</td>\n",
       "      <td>1472594459</td>\n",
       "      <td>20160830</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>ORD201608301546</td>\n",
       "      <td>Google Stretch Fit Hat M/L Navy</td>\n",
       "      <td>GGOEGHPL003214</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>M/L</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>29.49</td>\n",
       "      <td>21.99</td>\n",
       "      <td>21.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1472604143</td>\n",
       "      <td>1472604143</td>\n",
       "      <td>20160830</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ORD201608301559</td>\n",
       "      <td>Google 17oz Stainless Steel Sport Bottle</td>\n",
       "      <td>GGOEGDHC074099</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Single Option Only</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>33.89</td>\n",
       "      <td>18.99</td>\n",
       "      <td>18.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   visitNumber     visitId  visitStartTime      date  total_visits  \\\n",
       "0            2  1472571985      1472571985  20160830             1   \n",
       "1            1  1472606991      1472606991  20160830             1   \n",
       "2            3  1472599227      1472599227  20160830             1   \n",
       "3            1  1472594459      1472594459  20160830             1   \n",
       "4            3  1472604143      1472604143  20160830             1   \n",
       "\n",
       "   total_hits  total_pageviews  total_timeOnSite  total_bounces  \\\n",
       "0          13               11               305            NaN   \n",
       "1          13               12               392            NaN   \n",
       "2          14               12               192            NaN   \n",
       "3          16               14               685            NaN   \n",
       "4          16               13               895            NaN   \n",
       "\n",
       "   total_newVisits  ...  itemTransactionId  \\\n",
       "0              NaN  ...    ORD201608301467   \n",
       "1              1.0  ...    ORD201608301495   \n",
       "2              NaN  ...    ORD201608301510   \n",
       "3              1.0  ...    ORD201608301546   \n",
       "4              NaN  ...    ORD201608301559   \n",
       "\n",
       "                                ProductName      ProductSKU   Category  \\\n",
       "0      24 oz YouTube Sergeant Stripe Bottle  GGOEYDHJ019399  (not set)   \n",
       "1     Google Laptop and Cell Phone Stickers  GGOEGFKQ020399  (not set)   \n",
       "2   20 oz Stainless Steel Insulated Tumbler  GGOEGDHQ014899  (not set)   \n",
       "3           Google Stretch Fit Hat M/L Navy  GGOEGHPL003214  (not set)   \n",
       "4  Google 17oz Stainless Steel Sport Bottle  GGOEGDHC074099  (not set)   \n",
       "\n",
       "              Variant ProductList  Revenue   TotalCost   Price  Quantity  \n",
       "0  Single Option Only   (not set)    20.91        7.99    7.99         1  \n",
       "1  Single Option Only   (not set)   112.00       99.50    1.99        50  \n",
       "2  Single Option Only   (not set)    11.24       19.99   19.99         1  \n",
       "3                 M/L   (not set)    29.49       21.99   21.99         1  \n",
       "4  Single Option Only   (not set)    33.89       18.99   18.99         1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataframe contains a description of all sifferent features that we can use for creating a valid analysis and further (the recommendation system). The most important part is that this data is obtained from a Google's BigQuery use case in this [Kaggle](https://www.kaggle.com/code/paultimothymooney/how-to-query-the-google-analytics-sample-dataset/notebook?scriptVersionId=5165120)\n",
    "\n",
    "The most important features were selected from a list of features. I used a query to retrieve the products that were clicked by the users, as well as the confirmed purchases. This, along side some behavioral features like the time on which the users entered the site, the page views, time on site and other, all this can allow us to create behavior clusters in which we can create a user classification system and later classify behaviors into this already formed groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df.drop_duplicates(\n",
    "    subset=\"transactionId\", keep=\"first\", inplace=True\n",
    ")  # Drop duplicate orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google Men's 100% Cotton Short Sleeve Hero Tee Navy     30\n",
       "Google Men's  Zip Hoodie                                30\n",
       "Google Men's 100% Cotton Short Sleeve Hero Tee Black    27\n",
       "Google Men's 100% Cotton Short Sleeve Hero Tee White    25\n",
       "Engraved Ceramic Google Mug                             23\n",
       "                                                        ..\n",
       "Google Executive Umbrella                                1\n",
       "Google Accent Insulated Stainless Steel Bottle           1\n",
       "Gel Roller Pen                                           1\n",
       "Google Men's Performance Tee Gunmetal                    1\n",
       "Bic Digital Clic Stic Stylus Pen                         1\n",
       "Name: ProductName, Length: 248, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_df[\"ProductName\"].value_counts()  # Products that have being purchased before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mistral Rucksack                                          633\n",
       "Deluge Waterproof Backpack                                575\n",
       "Oasis Backpack                                            553\n",
       "Keyboard DOT Sticker                                      552\n",
       "Google Men's 100% Cotton Short Sleeve Hero Tee White      466\n",
       "                                                         ... \n",
       "Google Toddler Tee Fruit Games Cherries                     3\n",
       "Google Men's Short Sleeve Performance Badge Tee Pewter      2\n",
       "Gift Card - $250.00                                         1\n",
       "Google Toddler Tee Fruit Games Lemon                        1\n",
       "Google Youth Tee Fruit Games Pineapple                      1\n",
       "Name: ProductName, Length: 266, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df[\"ProductName\"].value_counts()  # Products that have being clicked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to see how variables like Hits and Time on Site are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>total_hits</th>\n",
       "      <th>Price</th>\n",
       "      <th>total_timeOnSite</th>\n",
       "      <th>total_newVisits</th>\n",
       "      <th>total_pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20878.00</td>\n",
       "      <td>20878.0</td>\n",
       "      <td>20878.00</td>\n",
       "      <td>20878.00</td>\n",
       "      <td>20863.00</td>\n",
       "      <td>11736.0</td>\n",
       "      <td>20878.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.96</td>\n",
       "      <td>28.11</td>\n",
       "      <td>923.26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.92</td>\n",
       "      <td>30.15</td>\n",
       "      <td>1060.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>243.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>16.99</td>\n",
       "      <td>547.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.00</td>\n",
       "      <td>34.99</td>\n",
       "      <td>1210.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>301.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>7508.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>188.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       visitNumber  total_visits  total_hits     Price  total_timeOnSite  \\\n",
       "count     20878.00       20878.0    20878.00  20878.00          20863.00   \n",
       "mean          4.22           1.0       50.96     28.11            923.26   \n",
       "std          13.46           0.0       51.92     30.15           1060.52   \n",
       "min           1.00           1.0        2.00      0.79              1.00   \n",
       "25%           1.00           1.0       17.00      5.99            243.00   \n",
       "50%           1.00           1.0       32.00     16.99            547.00   \n",
       "75%           3.00           1.0       65.00     34.99           1210.00   \n",
       "max         156.00           1.0      301.00    250.00           7508.00   \n",
       "\n",
       "       total_newVisits  total_pageviews  \n",
       "count          11736.0         20878.00  \n",
       "mean               1.0            33.45  \n",
       "std                0.0            32.42  \n",
       "min                1.0             1.00  \n",
       "25%                1.0            12.00  \n",
       "50%                1.0            22.00  \n",
       "75%                1.0            43.00  \n",
       "max                1.0           188.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = [\n",
    "    \"visitNumber\",\n",
    "    \"total_visits\",\n",
    "    \"total_hits\",\n",
    "    \"Price\",\n",
    "    \"total_timeOnSite\",\n",
    "    \"total_newVisits\",\n",
    "    \"total_pageviews\",\n",
    "]\n",
    "\n",
    "\n",
    "clicks_df.loc[:, features_list].describe().round(\n",
    "    2\n",
    ")  # This would be the behavioral features that we have available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Embeddings \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system will work with what is called embedding a text in NLP. We take our text, map it into tokens, which are basically small id's for the word. Sometimes words are divided in multiple tokens. Which account for the fact that some words that are very similar or share a common root can be represented similarly. Those tokens are unique id's to the realm of what will be our embeddings realm. Which is nothing more than a vectorized space in which this tokens are represented in a numeric form --> $ \\text{token:} i \\rightarrow \\vec{w_i} = <0.423,3.141,.674, ..., n>$.\n",
    "\n",
    "When we have a sentence or a paragraph, embeddings are usually the average of all the tokens that we find in that structure, and there we have it, our embedding system. Usually this embeddings spaces are then trained on a corpus of text to represent different features. This is similar to what our brain does with sentences: we remember the nouns, the verbs, the grammar rules, and other complex rules that make our understanding of the language as it is.\n",
    "\n",
    "On this demo, we use ***multilingual-e5-base*** model for creating a 768 size vector that will account for the features in our product title, which is the main thing we want to use as a driver of recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from main.models import ModelClass\n",
    "\n",
    "model = ModelClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.35431285e-02,  4.46086712e-02, -4.48141305e-04,\n",
       "        -3.62625433e-04,  2.88973264e-02, -3.31031904e-02,\n",
       "        -1.90947279e-02, -3.67002678e-03,  3.32840942e-02,\n",
       "         3.20200399e-02, -2.73499619e-02, -1.29014198e-02,\n",
       "         1.63098887e-01,  3.99894156e-02, -3.45210619e-02,\n",
       "        -2.77194325e-02,  1.10267121e-02, -2.44755037e-02,\n",
       "         2.67019086e-02, -1.29492851e-02,  5.62458374e-02,\n",
       "        -2.56671403e-02,  3.23554426e-02, -3.01293898e-02,\n",
       "         2.17710696e-02, -9.27064288e-03,  2.39941813e-02,\n",
       "         1.11178188e-02, -3.06521188e-02,  3.21589038e-02,\n",
       "         9.67866369e-03, -7.68294372e-03,  3.73637117e-02,\n",
       "         1.61362011e-02,  3.65321301e-02,  4.55307253e-02,\n",
       "        -8.69155489e-03, -3.23016718e-02,  4.63906303e-02,\n",
       "         2.58311220e-02,  1.76636688e-02,  3.31863835e-02,\n",
       "        -2.47860118e-03, -4.25862223e-02,  2.99789384e-02,\n",
       "         2.67586531e-03,  1.81598496e-02,  1.37071135e-02,\n",
       "        -3.10147200e-02, -3.98573913e-02,  2.17524301e-02,\n",
       "         2.12536473e-03,  2.74068341e-02,  2.64701433e-02,\n",
       "        -4.24112715e-02, -7.41831586e-02,  3.37089896e-02,\n",
       "         2.91594844e-02, -1.08413678e-02,  4.31209840e-02,\n",
       "        -3.56764905e-02,  3.86151373e-02, -2.42685415e-02,\n",
       "         4.46611717e-02,  2.59732548e-02, -3.76304388e-02,\n",
       "        -1.67234279e-02,  8.91851541e-03, -4.96485308e-02,\n",
       "         3.86639964e-03,  2.11920287e-03, -2.36586854e-02,\n",
       "         3.81424055e-02, -5.30773923e-02, -5.24452142e-02,\n",
       "        -6.22227229e-03, -4.15331535e-02,  3.38769220e-02,\n",
       "        -1.78257823e-02, -1.35641862e-02,  2.85111722e-02,\n",
       "         5.53446449e-03,  6.90381527e-02,  5.10380380e-02,\n",
       "         9.22345556e-04, -1.09623149e-02, -2.87206303e-02,\n",
       "         2.59865243e-02,  1.69531852e-02,  4.01202440e-02,\n",
       "         3.01147960e-02,  1.59248356e-02, -2.82806084e-02,\n",
       "         4.11799885e-02,  2.87408475e-02,  1.10257789e-02,\n",
       "         1.26604345e-02,  3.80018540e-02,  2.69011892e-02,\n",
       "        -4.17140834e-02, -1.21648498e-02, -3.76928933e-02,\n",
       "        -4.06580046e-02, -3.81064452e-02, -4.26030643e-02,\n",
       "         1.28311766e-02, -6.96318131e-03, -2.01339014e-02,\n",
       "         2.86634322e-02, -1.10353883e-02, -4.41839248e-02,\n",
       "         7.80253089e-04,  4.27031666e-02, -4.57679220e-02,\n",
       "         1.61932278e-02, -3.51442881e-02,  4.67290059e-02,\n",
       "        -3.49800959e-02,  1.95586327e-02, -5.83028607e-02,\n",
       "         1.15779024e-02,  3.68077978e-02, -2.30936124e-03,\n",
       "        -1.13489351e-03,  3.52701880e-02, -3.39744166e-02,\n",
       "         1.20348157e-02, -8.30525253e-03,  5.10234982e-02,\n",
       "        -3.12388912e-02, -7.52537884e-03, -3.59107889e-02,\n",
       "        -3.04068048e-02,  2.86484044e-02, -6.03921711e-02,\n",
       "         3.26438658e-02,  1.69529431e-02,  1.32859107e-02,\n",
       "        -1.60200652e-02,  1.41947400e-02,  3.43857259e-02,\n",
       "        -5.68568408e-02,  4.33792435e-02,  4.36724164e-02,\n",
       "         4.44727018e-02, -6.13606051e-02,  3.20914760e-02,\n",
       "         1.83352437e-02, -1.46455085e-02,  4.76447716e-02,\n",
       "         2.61580627e-02, -3.03516649e-02,  2.96491925e-02,\n",
       "        -3.13770808e-02,  6.34351149e-02, -1.64898038e-02,\n",
       "        -6.18996732e-02, -1.27085065e-02, -1.17898202e-02,\n",
       "         2.84825489e-02,  3.77607755e-02,  8.30717478e-03,\n",
       "         1.36931930e-02, -1.04423938e-02,  4.41713035e-02,\n",
       "        -7.64082279e-03, -3.83477733e-02, -1.15521916e-03,\n",
       "        -3.97091266e-03, -3.93072404e-02,  4.22662962e-03,\n",
       "         6.24182355e-03, -6.86694831e-02, -7.61212502e-03,\n",
       "         3.85940337e-04, -1.79207586e-02, -6.84707658e-03,\n",
       "        -3.51754054e-02, -1.83149111e-02, -7.34719634e-02,\n",
       "        -4.98638079e-02, -2.94029526e-02, -2.03781724e-02,\n",
       "         3.74689922e-02,  6.00487413e-03, -1.49691114e-02,\n",
       "         5.38641878e-04, -1.24096056e-03,  5.12395892e-03,\n",
       "         2.87031960e-02, -5.54390019e-03,  5.48132621e-02,\n",
       "         2.72412337e-02,  2.44056657e-02,  8.10728874e-03,\n",
       "         3.41671593e-02,  4.05266993e-02,  2.17467993e-02,\n",
       "        -1.54777179e-02, -3.62418070e-02,  1.04224011e-02,\n",
       "         2.36579147e-03,  2.19174884e-02,  3.10999230e-02,\n",
       "        -4.46102470e-02, -6.96366653e-02,  2.27647256e-02,\n",
       "         4.07429971e-02,  2.98301764e-02,  4.24729735e-02,\n",
       "        -3.79574038e-02,  3.69497463e-02, -4.27811779e-02,\n",
       "         3.42831574e-02, -3.69573422e-02, -2.75723096e-02,\n",
       "         2.19730139e-02,  1.13826906e-02, -9.96753853e-03,\n",
       "         4.05241065e-02,  4.04196866e-02, -6.46519437e-02,\n",
       "         2.84010172e-02, -4.30664094e-03, -1.53392162e-02,\n",
       "         8.15628748e-03,  4.27987576e-02,  3.41311912e-03,\n",
       "         2.46511176e-02,  4.70294058e-03,  2.69533247e-02,\n",
       "         1.88580807e-02, -4.89721522e-02,  3.67790461e-02,\n",
       "        -2.47254409e-02, -5.21967746e-02,  3.88513841e-02,\n",
       "         5.21400943e-03, -4.77612168e-02, -1.16386227e-01,\n",
       "        -2.89213993e-02,  3.39829698e-02,  1.86222810e-02,\n",
       "        -2.20516417e-02,  2.70116683e-02, -4.44738083e-02,\n",
       "        -3.32826488e-02,  3.32029238e-02, -3.78521495e-02,\n",
       "         1.62873731e-03,  3.76106687e-02, -2.16027815e-02,\n",
       "        -7.15578394e-03, -2.51156837e-02,  4.71014790e-02,\n",
       "         1.31381685e-02, -3.69888283e-02,  1.26110762e-02,\n",
       "        -3.14338654e-02,  5.64537048e-02,  1.45712150e-02,\n",
       "        -4.17421088e-02, -2.96028331e-02, -2.27902941e-02,\n",
       "         5.34750521e-03,  4.17721793e-02,  1.04519188e-01,\n",
       "        -4.29859459e-02, -8.68200324e-03, -1.12690767e-02,\n",
       "        -3.31837609e-02, -1.18236449e-02, -8.16328749e-02,\n",
       "         5.41131794e-02,  1.71097722e-02, -3.83566618e-02,\n",
       "        -7.97246397e-02,  3.48219164e-02, -3.24652717e-02,\n",
       "        -1.99682862e-02, -3.19334120e-02,  5.68726398e-02,\n",
       "        -3.29810306e-02, -2.83374898e-02, -2.19120514e-02,\n",
       "         1.12266103e-02, -6.60369322e-02, -2.19742414e-02,\n",
       "        -9.83103812e-02,  1.90101750e-02,  4.75410484e-02,\n",
       "         6.09186776e-02, -3.50758210e-02,  4.68097115e-03,\n",
       "         5.66947497e-02,  6.53718086e-03,  5.07646501e-02,\n",
       "         5.48827276e-03, -1.14153279e-02,  3.70779727e-03,\n",
       "        -1.22275138e-02, -3.71391550e-02,  3.86273563e-02,\n",
       "        -3.72892395e-02, -3.97041067e-02,  3.72510999e-02,\n",
       "         8.87160003e-02,  6.86328905e-03, -4.46692146e-02,\n",
       "         4.90648150e-02, -1.40429968e-02, -3.23741399e-02,\n",
       "        -4.90797833e-02,  3.04208566e-02,  2.36685798e-02,\n",
       "        -4.83998843e-02,  4.48947288e-02,  5.26118614e-02,\n",
       "        -8.27535405e-05,  3.51345167e-02, -4.02962640e-02,\n",
       "         3.36265890e-03, -3.02178971e-02, -7.28529692e-02,\n",
       "         3.47969681e-02, -8.86912108e-04,  3.54850926e-02,\n",
       "        -6.67024264e-03,  7.18241632e-02,  4.99601942e-04,\n",
       "        -1.97659992e-02,  7.39906542e-03, -1.82707999e-02,\n",
       "         6.46907762e-02, -7.14164525e-02,  2.19787747e-04,\n",
       "        -6.01741346e-03, -1.52760195e-02,  6.71009198e-02,\n",
       "         2.60664709e-02, -4.77364957e-02,  2.35676393e-02,\n",
       "        -4.27062204e-03,  8.46965332e-03,  5.99004664e-02,\n",
       "        -2.96785366e-02, -1.12818833e-02,  1.60364900e-02,\n",
       "        -4.62773815e-02,  3.63326669e-02,  4.37067784e-02,\n",
       "        -4.68463860e-02,  3.87935191e-02, -3.18772654e-04,\n",
       "        -2.31666230e-02, -1.50328474e-02,  4.39406745e-02,\n",
       "        -2.62522418e-02, -2.60346886e-02, -4.85211723e-02,\n",
       "        -1.81245767e-02,  3.81502509e-02,  9.27473046e-03,\n",
       "        -1.15923854e-02,  3.77663062e-03,  4.12551165e-02,\n",
       "         1.00878365e-02, -3.55161652e-02,  2.58473884e-02,\n",
       "        -1.66486427e-02,  8.74106959e-02,  6.10421002e-02,\n",
       "        -2.81907395e-02, -1.98581126e-02,  3.60027589e-02,\n",
       "        -3.77626382e-02,  4.69816849e-02, -2.27004290e-02,\n",
       "        -2.21281853e-02,  5.06819263e-02,  8.11585113e-02,\n",
       "         4.67526764e-02,  4.37819436e-02,  1.48810726e-02,\n",
       "        -5.35603128e-02,  6.06984645e-02,  2.65982859e-02,\n",
       "        -6.08555116e-02,  4.45604473e-02, -2.57203430e-02,\n",
       "        -1.99204369e-04, -3.19867954e-02, -1.56608615e-02,\n",
       "         7.87417404e-03,  1.75184626e-02, -4.09222357e-02,\n",
       "         2.81936489e-02,  9.00178682e-03,  5.70053188e-03,\n",
       "        -4.06995304e-02,  1.90397929e-02,  1.40627492e-02,\n",
       "        -1.84969380e-02,  5.14450893e-02,  2.52811853e-02,\n",
       "         2.61611976e-02,  1.29898693e-02,  2.64872685e-02,\n",
       "        -1.11316396e-02, -1.86228063e-02,  5.23768216e-02,\n",
       "        -4.25021304e-03,  7.79516771e-02,  6.76082773e-03,\n",
       "        -7.85500556e-03, -4.01555710e-02, -1.79330353e-02,\n",
       "         5.11533320e-02,  6.31330982e-02,  3.29583064e-02,\n",
       "        -2.42038369e-02,  1.63941104e-02,  3.06018023e-03,\n",
       "         1.89776942e-02,  1.53171681e-02, -7.69889653e-02,\n",
       "        -2.97301114e-02, -1.79334525e-02,  1.76641140e-02,\n",
       "        -3.12399715e-02, -6.69604447e-03,  1.41358674e-02,\n",
       "        -1.68129597e-02,  4.90892446e-04,  3.83515805e-02,\n",
       "        -5.12668528e-02, -4.15194640e-03, -1.04806628e-02,\n",
       "        -2.22946852e-02,  2.45388933e-02, -1.74598433e-02,\n",
       "         2.17011776e-02,  5.05032996e-03, -1.20409066e-02,\n",
       "        -3.69507261e-02, -8.58487282e-03, -3.53234634e-02,\n",
       "        -9.50824656e-03, -9.20620002e-03, -1.58016067e-02,\n",
       "         2.29013618e-02,  1.29371388e-02,  1.25469938e-02,\n",
       "         9.62031307e-04,  3.32625955e-02, -6.22311123e-02,\n",
       "         3.57315354e-02, -2.22578347e-02,  2.25851573e-02,\n",
       "        -8.48543271e-02,  2.13052463e-02,  4.19902615e-02,\n",
       "        -2.41559092e-02, -2.99375560e-02, -3.52961309e-02,\n",
       "        -5.34577109e-02,  2.82900762e-02, -4.25953157e-02,\n",
       "        -8.71528778e-03, -6.08187448e-03, -4.42874134e-02,\n",
       "        -4.28284146e-02,  3.35756131e-02,  1.21015450e-02,\n",
       "         4.16371375e-02,  2.88969297e-02, -1.32345362e-02,\n",
       "         2.92665437e-02,  6.12936681e-03, -4.25501131e-02,\n",
       "         6.70707133e-03, -3.21449228e-02, -4.76803929e-02,\n",
       "         2.69552786e-02,  2.90341377e-02,  3.77134234e-02,\n",
       "        -6.48140609e-02,  6.34332672e-02, -6.41579926e-03,\n",
       "        -3.67261842e-02, -5.20555943e-04,  3.68459225e-02,\n",
       "         2.33043991e-02, -2.05731969e-02, -2.93197185e-02,\n",
       "         4.25572954e-02, -3.25288549e-02, -1.01914838e-01,\n",
       "         3.77049819e-02,  2.38660686e-02, -5.90828061e-03,\n",
       "        -2.50338893e-02, -4.38759625e-02,  2.00793110e-02,\n",
       "        -2.76546367e-02,  2.69819945e-02, -3.91320027e-02,\n",
       "        -2.05896758e-02,  6.09181449e-02, -1.92580149e-02,\n",
       "         1.22173592e-01, -6.28647357e-02, -2.23626681e-02,\n",
       "        -4.73226495e-02,  3.14763077e-02, -3.90565116e-03,\n",
       "        -1.37701686e-02,  2.71309260e-02, -4.87645390e-03,\n",
       "        -3.24032083e-02,  3.23030981e-03,  4.49450919e-03,\n",
       "        -4.97359186e-02, -1.67586487e-02,  3.33820097e-03,\n",
       "        -3.59689705e-02,  3.34765739e-03, -6.13355450e-02,\n",
       "         1.32793467e-02, -4.27174196e-02,  3.38858590e-02,\n",
       "        -2.63050813e-02, -5.72178839e-03, -8.04276764e-02,\n",
       "        -3.98828909e-02,  4.36572321e-02, -2.61800718e-02,\n",
       "        -5.22204442e-03,  4.91948649e-02,  5.45358611e-03,\n",
       "        -1.32349525e-02, -3.76902223e-02,  1.49169508e-02,\n",
       "        -2.51160972e-02,  2.03875881e-02, -4.29334790e-02,\n",
       "         1.21190576e-02, -6.96992408e-03,  6.15744619e-03,\n",
       "         8.88925605e-03,  7.87805393e-03, -2.28657871e-02,\n",
       "         2.10395921e-03,  5.79762720e-02, -4.00526747e-02,\n",
       "        -9.58744262e-04, -1.51073025e-03,  3.16491537e-02,\n",
       "         1.97774339e-02,  5.16659953e-03, -2.35147495e-02,\n",
       "        -1.57255400e-02, -6.41526747e-03,  3.01493816e-02,\n",
       "        -1.81233492e-02,  2.78482642e-02,  6.98550139e-03,\n",
       "        -2.48032399e-02, -4.35818881e-02,  6.27905279e-02,\n",
       "        -1.06851928e-01,  3.65137756e-02,  3.80581506e-02,\n",
       "        -1.98280644e-02,  4.24692184e-02,  2.59687006e-02,\n",
       "        -7.16818403e-03, -5.43215908e-02,  2.45442521e-02,\n",
       "        -5.96438423e-02, -3.98729295e-02, -3.14999837e-03,\n",
       "        -7.38725299e-03,  3.61230895e-02,  7.79104186e-03,\n",
       "         2.20862366e-02,  1.72962826e-02, -3.14909108e-02,\n",
       "        -2.15574801e-01,  3.41514647e-02, -1.31365769e-02,\n",
       "        -2.61950176e-02,  3.53886164e-04, -3.35468864e-03,\n",
       "        -1.75625328e-02,  1.96989290e-02,  2.36723982e-02,\n",
       "         2.09786296e-02,  3.74919921e-02, -8.02379176e-02,\n",
       "         5.81921861e-02, -1.39250364e-02, -3.95378098e-02,\n",
       "         2.18524542e-02, -6.27719052e-03, -4.64786403e-02,\n",
       "         1.70765538e-02,  1.17431525e-02, -2.95544206e-03,\n",
       "         5.63306771e-02, -3.45402509e-02, -2.42380369e-02,\n",
       "        -3.52785215e-02, -4.89148386e-02, -4.23150845e-02,\n",
       "         2.40509454e-02,  3.79912890e-02,  3.90286222e-02,\n",
       "         6.73666270e-03, -2.45051552e-03,  3.07383426e-02,\n",
       "         3.13777439e-02, -2.74343267e-02,  1.75019782e-02,\n",
       "        -3.03823575e-02,  6.63647382e-03, -6.97333738e-02,\n",
       "         1.12428172e-02,  4.21882933e-03, -6.88528940e-02,\n",
       "        -1.74538791e-02,  2.93735880e-02, -2.62463801e-02,\n",
       "         7.48093799e-02, -6.40728325e-03,  1.26053048e-02,\n",
       "        -3.08641922e-02, -1.97979268e-02,  5.37883006e-02,\n",
       "         3.04687563e-02, -8.92163739e-02, -2.13922095e-02,\n",
       "        -2.95033567e-02,  3.43570411e-02,  2.47887224e-02,\n",
       "         1.45063112e-02,  5.08262068e-02, -2.66155042e-02,\n",
       "         4.82789055e-02,  4.68810238e-02, -2.43621040e-02,\n",
       "         3.56855914e-02, -2.11119708e-02,  1.55909642e-04,\n",
       "         2.17895489e-02, -5.95876947e-02, -2.88250968e-02,\n",
       "         4.05047508e-03,  1.36619704e-02, -2.82033589e-02,\n",
       "        -3.26276906e-02,  5.80495074e-02,  4.65225391e-02,\n",
       "        -1.19214263e-02,  2.70192176e-02, -9.56492871e-03,\n",
       "        -8.33830014e-02,  2.98428088e-02, -1.66798774e-02,\n",
       "        -2.17798110e-02, -1.11693433e-02,  2.31061503e-02,\n",
       "        -1.68533567e-02, -3.26752625e-02,  3.62949334e-02,\n",
       "         8.28083139e-03,  3.16106863e-02,  1.60665512e-02,\n",
       "        -2.86038723e-02, -8.78037885e-03,  2.04741545e-02,\n",
       "        -1.61808496e-03,  1.76287033e-02, -4.60783346e-03,\n",
       "        -3.00847907e-02,  3.78556438e-02, -1.23188775e-02,\n",
       "        -2.71184538e-02,  2.83715571e-03, -1.27610546e-02,\n",
       "        -2.04636864e-02,  3.20107415e-02,  3.46112400e-02,\n",
       "        -3.11202146e-02, -4.66310121e-02,  4.73830886e-02,\n",
       "        -2.38422100e-02, -6.03785068e-02,  2.83239204e-02,\n",
       "         2.62020733e-02,  4.86632660e-02,  4.64331417e-04,\n",
       "         9.49457986e-04, -2.61395406e-02,  1.11149326e-02,\n",
       "        -2.80739050e-02, -3.09200641e-02, -3.93783785e-02,\n",
       "        -3.32290307e-02,  2.77442602e-03,  5.72455749e-02,\n",
       "         1.79268231e-04,  5.44776544e-02, -5.09269943e-04,\n",
       "        -8.44507199e-03,  2.63564475e-02,  3.28767449e-02,\n",
       "         3.28039043e-02, -1.51524972e-02, -4.54264432e-02,\n",
       "         7.78746679e-02, -2.27056090e-02, -5.41557744e-03,\n",
       "         4.83720042e-02,  3.23606580e-02, -2.39531528e-02,\n",
       "         2.69864406e-02,  2.15697903e-02,  7.40615353e-02,\n",
       "        -6.40474558e-02,  1.65803917e-03,  1.44278230e-02,\n",
       "        -4.08369564e-02,  1.95005704e-02,  1.96037032e-02,\n",
       "        -2.08869427e-02,  4.15495895e-02, -1.04100201e-02,\n",
       "        -2.50676759e-02, -8.32316577e-02, -3.38065065e-02,\n",
       "        -1.74092203e-02, -3.01162452e-02,  7.90571608e-03,\n",
       "         2.78408919e-03,  1.30682094e-02, -3.17530036e-02,\n",
       "         2.27898676e-02, -4.18325514e-02, -3.53863761e-02,\n",
       "         3.69474962e-02,  2.67261546e-02,  1.61593724e-02,\n",
       "         5.64749027e-03, -2.21584528e-03, -8.04931670e-03,\n",
       "         4.13561687e-02,  2.66142543e-02, -1.77253671e-02,\n",
       "        -9.25941672e-03,  4.87064049e-02,  5.44931414e-03,\n",
       "         2.55177058e-02, -1.64157581e-02,  4.74485010e-02,\n",
       "        -5.66086471e-02, -5.27970120e-02,  3.62193026e-02]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.create_embeddings(\"Hello world\")  # Use method create embeddings to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we would either need to run this over all the dataframes rows, which is not wrong, but will take some time that we need ;)\n",
    "\n",
    "Therefore we are going to create the embeddings in a separate file, associated with a title. Since there are onlt 266 products, we dont need more than that. But now is the part where we make our first decision. Either we do some feature engineering for creating a new df which contains unique visitID clicks, append all the products clicked in just one big string, and then perform the embeddings process, or, we can create the embeddings for the 266 products and then create some average. The first process seems more accurate but will take longer, and the second one is going to take less but could be less straightforward. \n",
    "\n",
    "We are going then to take both approaches to see where both go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Approach - Concatenate Likes and Embed the  \"Like List\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         visitId                                             Clicks\n",
      "0     1470035161                                  Google Power Bank\n",
      "1     1470035457                         Deluge Waterproof Backpack\n",
      "2     1470035521  Seat Pack Organizer, Seat Pack Organizer, Seat...\n",
      "3     1470035892  Google Men's 100% Cotton Short Sleeve Hero Tee...\n",
      "4     1470036291  Google Men's 100% Cotton Short Sleeve Hero Tee...\n",
      "...          ...                                                ...\n",
      "4630  1470896943  Google Toddler Tee Fruit Games Pineapple, Goog...\n",
      "4631  1470896966  Google Canvas Tote Natural/Navy, Mistral Rucks...\n",
      "4632  1470897377      Google Women's Lightweight Microfleece Jacket\n",
      "4633  1470897905  Google Men's 100% Cotton Short Sleeve Hero Tee...\n",
      "4634  1470897937          Google Women's Performance Golf Polo Blue\n",
      "\n",
      "[4635 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by 'visitId' and aggregate 'Product name' using text join\n",
    "grouped_df = (\n",
    "    df.groupby(\"visitId\")[\"ProductName\"].apply(lambda x: \", \".join(x)).reset_index()\n",
    ")\n",
    "\n",
    "# Rename the aggregated column\n",
    "grouped_df.rename(columns={\"ProductName\": \"Clicks\"}, inplace=True)\n",
    "\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()  # fancy progress Bar\n",
    "\n",
    "grouped_df[\"Embedings\"] = grouped_df.Clicks.progress_apply(\n",
    "    lambda x: model.create_embeddings(x)\n",
    ")  # Make the embeddings for the Clicks column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.021230968, 0.06723951, -0.03646528, -0.005...\n",
       "1       [[0.021318657, 0.04240716, -0.007994821, 0.004...\n",
       "2       [[0.002186621, 0.051950835, -0.004776678, 0.02...\n",
       "3       [[0.00616312, 0.04824848, -0.035558887, 0.0190...\n",
       "4       [[-9.8550074e-05, 0.034511797, -0.016930008, 0...\n",
       "                              ...                        \n",
       "4630    [[0.0056050452, 0.029619096, -0.017937213, 0.0...\n",
       "4631    [[0.02744697, 0.05886887, -0.022487251, 0.0082...\n",
       "4632    [[0.010117159, 0.042951882, -0.020193057, -0.0...\n",
       "4633    [[0.0072451867, 0.036614206, -0.039741036, 0.0...\n",
       "4634    [[0.02905081, 0.046681486, -0.016705029, 0.021...\n",
       "Name: Embeddings, Length: 4635, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.Embeddings  # The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Approach - Create our Embeddings Master File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()\n",
    "\n",
    "inv_df = pd.DataFrame(\n",
    "    [clicks_df.ProductName.unique(), clicks_df[\"ProductName\"].value_counts()]\n",
    ").T  # Products that have being clicked\n",
    "inv_df.rename({0: \"Product\", 1: \"Count\"}, axis=1, inplace=True)\n",
    "\n",
    "inv_df[\"Embeddings\"] = inv_df.Product.progress_apply(model.create_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[0.014249629, 0.023254616, -0.006114072, 0.00...\n",
       "1      [[0.03786302, 0.03239363, -0.00020421806, 0.02...\n",
       "2      [[0.00114997, 0.03650656, -0.035562657, 0.0202...\n",
       "3      [[0.028030083, 0.038984273, -0.02813911, 0.034...\n",
       "4      [[0.015068452, 0.05567977, -0.016779918, 0.011...\n",
       "                             ...                        \n",
       "261    [[0.021007422, 0.04520183, -0.023061385, 0.041...\n",
       "262    [[0.013173798, 0.00889726, -0.012541152, 0.015...\n",
       "263    [[0.008392215, 0.055684857, -0.011121839, -0.0...\n",
       "264    [[-0.01466453, 0.04144204, -0.04656081, 0.0284...\n",
       "265    [[0.0017286192, 0.04235564, -0.026112005, 0.01...\n",
       "Name: Embeddings, Length: 266, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_df.Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Embeddings_x</th>\n",
       "      <th>Embeddings_y</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>date</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>total_hits</th>\n",
       "      <th>total_pageviews</th>\n",
       "      <th>...</th>\n",
       "      <th>Category</th>\n",
       "      <th>Variant</th>\n",
       "      <th>ProductList</th>\n",
       "      <th>transactionId</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>itemTransactionId</th>\n",
       "      <th>Click</th>\n",
       "      <th>TotalPaid</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oasis Backpack</td>\n",
       "      <td>[[0.014249629, 0.023254616, -0.006114072, 0.00...</td>\n",
       "      <td>[[0.014249629, 0.023254616, -0.006114072, 0.00...</td>\n",
       "      <td>2</td>\n",
       "      <td>1470857552</td>\n",
       "      <td>1470857552</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Limited Supply/Bags/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Set of 3 Nested Travel Cases</td>\n",
       "      <td>[[0.03786302, 0.03239363, -0.00020421806, 0.02...</td>\n",
       "      <td>[[0.03786302, 0.03239363, -0.00020421806, 0.02...</td>\n",
       "      <td>1</td>\n",
       "      <td>1470862136</td>\n",
       "      <td>1470862136</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Limited Supply/Bags/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.79</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouTube Men's Short Sleeve Hero Tee Black</td>\n",
       "      <td>[[0.00114997, 0.03650656, -0.035562657, 0.0202...</td>\n",
       "      <td>[[0.00114997, 0.03650656, -0.035562657, 0.0202...</td>\n",
       "      <td>1</td>\n",
       "      <td>1470848713</td>\n",
       "      <td>1470848713</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Men's/Men's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maze Pen</td>\n",
       "      <td>[[0.028030083, 0.038984273, -0.02813911, 0.034...</td>\n",
       "      <td>[[0.028030083, 0.038984273, -0.02813911, 0.034...</td>\n",
       "      <td>4</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Office/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maze Pen</td>\n",
       "      <td>[[0.028030083, 0.038984273, -0.02813911, 0.034...</td>\n",
       "      <td>[[0.028030083, 0.038984273, -0.02813911, 0.034...</td>\n",
       "      <td>4</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>1470815734</td>\n",
       "      <td>20160810</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Office/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20873</th>\n",
       "      <td>Google Women's Hero V-Neck Tee White</td>\n",
       "      <td>[[0.018843824, 0.053733025, -0.02561408, 0.008...</td>\n",
       "      <td>[[0.018843824, 0.053733025, -0.02561408, 0.008...</td>\n",
       "      <td>7</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>20160801</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Women's/Women's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874</th>\n",
       "      <td>Google Women's Hero V-Neck Tee White</td>\n",
       "      <td>[[0.018843824, 0.053733025, -0.02561408, 0.008...</td>\n",
       "      <td>[[0.018843824, 0.053733025, -0.02561408, 0.008...</td>\n",
       "      <td>7</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>20160801</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Women's/Women's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20875</th>\n",
       "      <td>Google Women's Short Sleeve Hero Tee White</td>\n",
       "      <td>[[-5.1616345e-05, 0.047178693, -0.030958962, 0...</td>\n",
       "      <td>[[-5.1616345e-05, 0.047178693, -0.030958962, 0...</td>\n",
       "      <td>7</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>20160801</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Women's/Women's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20876</th>\n",
       "      <td>Google Women's Short Sleeve V-Neck Tee Lavender</td>\n",
       "      <td>[[0.01456775, 0.050580125, -0.034778133, 0.027...</td>\n",
       "      <td>[[0.01456775, 0.050580125, -0.034778133, 0.027...</td>\n",
       "      <td>7</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>20160801</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Women's/Women's-T-Shirts/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20877</th>\n",
       "      <td>Google Women's Fleece Hoodie</td>\n",
       "      <td>[[0.01493587, 0.05140108, -0.021872329, 0.0094...</td>\n",
       "      <td>[[0.01493587, 0.05140108, -0.021872329, 0.0094...</td>\n",
       "      <td>7</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>1470120798</td>\n",
       "      <td>20160801</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Home/Apparel/Women's/Women's-Outerwear/</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20878 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Product  \\\n",
       "0                                       Oasis Backpack   \n",
       "1                         Set of 3 Nested Travel Cases   \n",
       "2            YouTube Men's Short Sleeve Hero Tee Black   \n",
       "3                                             Maze Pen   \n",
       "4                                             Maze Pen   \n",
       "...                                                ...   \n",
       "20873             Google Women's Hero V-Neck Tee White   \n",
       "20874             Google Women's Hero V-Neck Tee White   \n",
       "20875       Google Women's Short Sleeve Hero Tee White   \n",
       "20876  Google Women's Short Sleeve V-Neck Tee Lavender   \n",
       "20877                     Google Women's Fleece Hoodie   \n",
       "\n",
       "                                            Embeddings_x  \\\n",
       "0      [[0.014249629, 0.023254616, -0.006114072, 0.00...   \n",
       "1      [[0.03786302, 0.03239363, -0.00020421806, 0.02...   \n",
       "2      [[0.00114997, 0.03650656, -0.035562657, 0.0202...   \n",
       "3      [[0.028030083, 0.038984273, -0.02813911, 0.034...   \n",
       "4      [[0.028030083, 0.038984273, -0.02813911, 0.034...   \n",
       "...                                                  ...   \n",
       "20873  [[0.018843824, 0.053733025, -0.02561408, 0.008...   \n",
       "20874  [[0.018843824, 0.053733025, -0.02561408, 0.008...   \n",
       "20875  [[-5.1616345e-05, 0.047178693, -0.030958962, 0...   \n",
       "20876  [[0.01456775, 0.050580125, -0.034778133, 0.027...   \n",
       "20877  [[0.01493587, 0.05140108, -0.021872329, 0.0094...   \n",
       "\n",
       "                                            Embeddings_y  visitNumber  \\\n",
       "0      [[0.014249629, 0.023254616, -0.006114072, 0.00...            2   \n",
       "1      [[0.03786302, 0.03239363, -0.00020421806, 0.02...            1   \n",
       "2      [[0.00114997, 0.03650656, -0.035562657, 0.0202...            1   \n",
       "3      [[0.028030083, 0.038984273, -0.02813911, 0.034...            4   \n",
       "4      [[0.028030083, 0.038984273, -0.02813911, 0.034...            4   \n",
       "...                                                  ...          ...   \n",
       "20873  [[0.018843824, 0.053733025, -0.02561408, 0.008...            7   \n",
       "20874  [[0.018843824, 0.053733025, -0.02561408, 0.008...            7   \n",
       "20875  [[-5.1616345e-05, 0.047178693, -0.030958962, 0...            7   \n",
       "20876  [[0.01456775, 0.050580125, -0.034778133, 0.027...            7   \n",
       "20877  [[0.01493587, 0.05140108, -0.021872329, 0.0094...            7   \n",
       "\n",
       "          visitId  visitStartTime      date  total_visits  total_hits  \\\n",
       "0      1470857552      1470857552  20160810             1           5   \n",
       "1      1470862136      1470862136  20160810             1           5   \n",
       "2      1470848713      1470848713  20160810             1           6   \n",
       "3      1470815734      1470815734  20160810             1           6   \n",
       "4      1470815734      1470815734  20160810             1           6   \n",
       "...           ...             ...       ...           ...         ...   \n",
       "20873  1470120798      1470120798  20160801             1          34   \n",
       "20874  1470120798      1470120798  20160801             1          34   \n",
       "20875  1470120798      1470120798  20160801             1          34   \n",
       "20876  1470120798      1470120798  20160801             1          34   \n",
       "20877  1470120798      1470120798  20160801             1          34   \n",
       "\n",
       "       total_pageviews  ...                                 Category  \\\n",
       "0                    4  ...                Home/Limited Supply/Bags/   \n",
       "1                    4  ...                Home/Limited Supply/Bags/   \n",
       "2                    5  ...       Home/Apparel/Men's/Men's-T-Shirts/   \n",
       "3                    3  ...                             Home/Office/   \n",
       "4                    3  ...                             Home/Office/   \n",
       "...                ...  ...                                      ...   \n",
       "20873               18  ...   Home/Apparel/Women's/Women's-T-Shirts/   \n",
       "20874               18  ...   Home/Apparel/Women's/Women's-T-Shirts/   \n",
       "20875               18  ...   Home/Apparel/Women's/Women's-T-Shirts/   \n",
       "20876               18  ...   Home/Apparel/Women's/Women's-T-Shirts/   \n",
       "20877               18  ...  Home/Apparel/Women's/Women's-Outerwear/   \n",
       "\n",
       "         Variant  ProductList  transactionId Revenue  itemTransactionId  \\\n",
       "0      (not set)     Category            NaN     0.0                NaN   \n",
       "1      (not set)     Category            NaN     0.0                NaN   \n",
       "2      (not set)     Category            NaN     0.0                NaN   \n",
       "3      (not set)     Category            NaN     0.0                NaN   \n",
       "4      (not set)     Category            NaN     0.0                NaN   \n",
       "...          ...          ...            ...     ...                ...   \n",
       "20873  (not set)    (not set)            NaN     0.0                NaN   \n",
       "20874  (not set)    (not set)            NaN     0.0                NaN   \n",
       "20875  (not set)    (not set)            NaN     0.0                NaN   \n",
       "20876  (not set)    (not set)            NaN     0.0                NaN   \n",
       "20877  (not set)    (not set)            NaN     0.0                NaN   \n",
       "\n",
       "       Click TotalPaid  Price Quantity  \n",
       "0       True       0.0  99.99      NaN  \n",
       "1       True       0.0   8.79      NaN  \n",
       "2       True       0.0  16.99      NaN  \n",
       "3       True       0.0   0.99      NaN  \n",
       "4       True       0.0   0.99      NaN  \n",
       "...      ...       ...    ...      ...  \n",
       "20873   True       0.0  16.99      NaN  \n",
       "20874   True       0.0  16.99      NaN  \n",
       "20875   True       0.0  17.99      NaN  \n",
       "20876   True       0.0  16.99      NaN  \n",
       "20877   True       0.0  55.99      NaN  \n",
       "\n",
       "[20878 rows x 36 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now use this inv_df and original df to make a new df containing the embeddings.\n",
    "\n",
    "clicks_df.rename({\"ProductName\": \"Product\"}, axis=1, inplace=True)\n",
    "\n",
    "clicks_df = pd.merge(\n",
    "    inv_df[[\"Product\", \"Embeddings\"]], clicks_df, on=\"Product\", how=\"right\")\n",
    "\n",
    "clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Now we store pur embeddings so that we dont do the same all over again\n",
    "\n",
    "import os\n",
    "\n",
    "inv_df.to_hdf(\"google data/embeddings/inventory_embeddings.h5\", key=\"data\", mode=\"w\") \n",
    "grouped_df.to_hdf(\"google data/embeddings/likes_embeddings.h5\", key=\"data\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Second phase of the walkthrough here with the embeddings completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start directly her eif the embeddings is not in what we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement something that creates a scoring based on the behavioral features. This way, the recommendation process will have more sense. In this use case, we are going to use the original to extract a scoring function using a \"Sell\" as the target variable that we want to predict. This is the general objective of the e-commerce use case. You may think of how are we going to create a score based on this. \n",
    "\n",
    "The answer comes as a trick in transforming the data. We are going to take one set of data that contains visit ID who end up in purchases and cases in which are not. Then using a classic sigmoid head at the end of whatever our model is going to be, we end up getting the probabilities of one visit ID ending up in a purchase, which turns this also into a binary classification problem. Our score is going to be the probability that ends up in a selling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "emb = pd.read_hdf('google data/embeddings/inventory_embeddings.h5',key='data') # Embeddings of the inventory. \n",
    "\n",
    "buy_df = pd.read_csv(\"google data/purchases.csv\") # only purchases \n",
    "data = pd.read_csv(\"google data/data.csv\")        # mixed df\n",
    "data = data.loc[data.Click == True]\n",
    "\n",
    "data.rename({\"ProductName\": \"Product\"}, axis=1, inplace=True)\n",
    "buy_df.rename({\"ProductName\": \"Product\"}, axis=1, inplace=True)\n",
    "\n",
    "data = pd.merge(\n",
    "    emb[[\"Product\", \"Embeddings\"]], data, on=\"Product\", how=\"right\") # Add the embeddings Column\n",
    "\n",
    "buy_df = pd.merge( \n",
    "    emb[[\"Product\", \"Embeddings\"]], buy_df, on=\"Product\", how=\"right\") # Add the embeddings Column\n",
    "\n",
    "\n",
    "data.dropna(subset='Embeddings',inplace=True)\n",
    "buy_df.dropna(subset='Embeddings',inplace=True)\n",
    "\n",
    "\n",
    "features_list = [\n",
    "    \"visitId\",\n",
    "    \"visitNumber\",\n",
    "    \"total_visits\",\n",
    "    \"total_hits\",\n",
    "    \"Price\",\n",
    "    \"total_timeOnSite\",\n",
    "    \"total_newVisits\",\n",
    "    \"total_pageviews\",\n",
    "]\n",
    "\n",
    "## TRANSFORMATIONS -------------------\n",
    "\n",
    "# Group by 'visitId' and aggregate 'Product name' using text join\n",
    "\n",
    "data_x =  buy_df.groupby(\"visitId\")[\"Product\"].apply(lambda x: \", \".join(x)).reset_index()\n",
    "data_xx = data.groupby(\"visitId\")[\"Product\"].apply(lambda x: \", \".join(x)).reset_index()\n",
    "\n",
    "# Group by visitid and aggregate 'Revenue'\n",
    "\n",
    "data_y = buy_df.groupby(\"visitId\")['Revenue'].apply(lambda x: 1 if np.sum(x)>0 else 0).reset_index()\n",
    "data_yy =  data.groupby(\"visitId\")['Revenue'].apply(lambda x: 1 if np.sum(x)>0 else 0).reset_index()\n",
    "\n",
    "# Combine the embeddings of all the products bought or clicked in the same session\n",
    "\n",
    "emb1 = buy_df.groupby(\"visitId\")['Embeddings'].apply(lambda x: np.stack(x,axis=0)).reset_index()\n",
    "emb2 = data.groupby(\"visitId\")['Embeddings'].apply(lambda x: np.stack(x,axis=0)).reset_index()\n",
    "\n",
    "# Rename the aggregated column\n",
    "\n",
    "data_x.rename(columns={\"ProductName\": \"Clicks\"}, inplace=True)\n",
    "data_y.rename(columns={\"Revenue\": \"Sells\"}, inplace=True)\n",
    "data_xx.rename(columns={\"ProductName\": \"Clicks\"}, inplace=True)\n",
    "data_yy.rename(columns={\"Revenue\": \"Sells\"}, inplace=True)\n",
    "\n",
    "buy_df = (\n",
    "    pd.merge(data_x, buy_df.loc[:, features_list], on=\"visitId\", how=\"left\")\n",
    "    .drop_duplicates(subset=\"visitId\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "data = (\n",
    "    pd.merge(data_xx, data.loc[:, features_list], on=\"visitId\", how=\"left\")\n",
    "    .drop_duplicates(subset=\"visitId\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "buy_df.total_newVisits =  buy_df.total_newVisits.apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "data.total_newVisits =  data.total_newVisits.apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "buy_df = pd.merge(data_y, buy_df,on=\"visitId\", how=\"right\")  # DF with NOTHING MORE THAN COMPLETED ORDERS\n",
    "data = pd.merge(data_yy, data,on=\"visitId\", how=\"right\")  # DF with NOTHING MORE THAN COMPLETED ORDERS\n",
    "\n",
    "# Append the embeddings \n",
    "buy_df = pd.merge(emb1, buy_df,on=\"visitId\", how=\"right\")\n",
    "data = pd.merge(emb2, data,on=\"visitId\", how=\"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4614.000000\n",
      "mean        4.524924\n",
      "std         6.155768\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%         5.000000\n",
      "max       111.000000\n",
      "Name: Embeddings, dtype: float64 \n",
      "\n",
      "count    1016.000000\n",
      "mean        3.375000\n",
      "std         3.215131\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         4.000000\n",
      "max        27.000000\n",
      "Name: Embeddings, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "size = data.Embeddings.apply(lambda x: x.shape[0])\n",
    "size_ = buy_df.Embeddings.apply(lambda x: x.shape[0])\n",
    "\n",
    "print(size.describe(),\"\\n\")  # We can see here that instead of just getting the mean between the things viewed or bought on each session. We could stack along a new dimension for working in a more complex way with them.\n",
    "print(size_.describe())      # Since the distributions indicate that around 5 objects or items is the most common number, we can either trim or pad to adjust to that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sessions in which one order was completed in the first data source is: 0 from 4614 instances.\n",
      "the number of sessions in which one order was completed in the second data source is: 1016 from 1016 instances.\n"
     ]
    }
   ],
   "source": [
    "print(f\"the number of sessions in which one order was completed in the first data source is: {(data.Sells==1).sum()} from {data.shape[0]} instances.\" )\n",
    "print(f\"the number of sessions in which one order was completed in the second data source is: {(buy_df.Sells==1).sum()} from {buy_df.shape[0]} instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitId</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>Sells</th>\n",
       "      <th>Product</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>total_hits</th>\n",
       "      <th>Price</th>\n",
       "      <th>total_timeOnSite</th>\n",
       "      <th>total_newVisits</th>\n",
       "      <th>total_pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1470035161</td>\n",
       "      <td>[[[0.021230968, 0.06723951, -0.03646528, -0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>Google Power Bank</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>16.99</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1470035457</td>\n",
       "      <td>[[[0.021318657, 0.04240716, -0.007994821, 0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>Deluge Waterproof Backpack</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>99.99</td>\n",
       "      <td>421.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1470035521</td>\n",
       "      <td>[[[0.015425235, 0.023942046, 0.0014200645, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>Seat Pack Organizer, Seat Pack Organizer, Seat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>11.99</td>\n",
       "      <td>755.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1470035892</td>\n",
       "      <td>[[[0.00616312, 0.04824848, -0.035558887, 0.019...</td>\n",
       "      <td>0</td>\n",
       "      <td>Google Men's 100% Cotton Short Sleeve Hero Tee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>16.99</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1470036291</td>\n",
       "      <td>[[[0.007224333, 0.037768986, -0.038783304, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>Google Men's 100% Cotton Short Sleeve Hero Tee...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>16.99</td>\n",
       "      <td>465.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>1472602179</td>\n",
       "      <td>[[[0.0111623695, 0.0331794, -0.040837016, 0.01...</td>\n",
       "      <td>1</td>\n",
       "      <td>Google Men's 100% Cotton Short Sleeve Hero Tee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>16.99</td>\n",
       "      <td>494.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>1472604143</td>\n",
       "      <td>[[[0.01618643, 0.05558616, -0.027737834, -0.00...</td>\n",
       "      <td>1</td>\n",
       "      <td>Google 17oz Stainless Steel Sport Bottle</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>18.99</td>\n",
       "      <td>895.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>1472606991</td>\n",
       "      <td>[[[0.011397464, 0.06847937, -0.0370449, -0.012...</td>\n",
       "      <td>1</td>\n",
       "      <td>Google Laptop and Cell Phone Stickers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.99</td>\n",
       "      <td>392.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>1472610310</td>\n",
       "      <td>[[[0.0105100805, 0.07540334, -0.035388947, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>Google Canvas Tote Natural/Navy, Badge Holder,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>12.79</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>1472615797</td>\n",
       "      <td>[[[0.0072451867, 0.036614206, -0.039741036, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>Google Men's 100% Cotton Short Sleeve Hero Tee...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>13.59</td>\n",
       "      <td>568.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5381 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         visitId                                         Embeddings  Sells  \\\n",
       "0     1470035161  [[[0.021230968, 0.06723951, -0.03646528, -0.00...      0   \n",
       "1     1470035457  [[[0.021318657, 0.04240716, -0.007994821, 0.00...      0   \n",
       "2     1470035521  [[[0.015425235, 0.023942046, 0.0014200645, 0.0...      0   \n",
       "3     1470035892  [[[0.00616312, 0.04824848, -0.035558887, 0.019...      0   \n",
       "4     1470036291  [[[0.007224333, 0.037768986, -0.038783304, 0.0...      0   \n",
       "...          ...                                                ...    ...   \n",
       "5625  1472602179  [[[0.0111623695, 0.0331794, -0.040837016, 0.01...      1   \n",
       "5626  1472604143  [[[0.01618643, 0.05558616, -0.027737834, -0.00...      1   \n",
       "5627  1472606991  [[[0.011397464, 0.06847937, -0.0370449, -0.012...      1   \n",
       "5628  1472610310  [[[0.0105100805, 0.07540334, -0.035388947, 0.0...      1   \n",
       "5629  1472615797  [[[0.0072451867, 0.036614206, -0.039741036, 0....      1   \n",
       "\n",
       "                                                Product  visitNumber  \\\n",
       "0                                     Google Power Bank            1   \n",
       "1                            Deluge Waterproof Backpack            3   \n",
       "2     Seat Pack Organizer, Seat Pack Organizer, Seat...            1   \n",
       "3     Google Men's 100% Cotton Short Sleeve Hero Tee...            1   \n",
       "4     Google Men's 100% Cotton Short Sleeve Hero Tee...            7   \n",
       "...                                                 ...          ...   \n",
       "5625  Google Men's 100% Cotton Short Sleeve Hero Tee...            1   \n",
       "5626           Google 17oz Stainless Steel Sport Bottle            3   \n",
       "5627              Google Laptop and Cell Phone Stickers            1   \n",
       "5628  Google Canvas Tote Natural/Navy, Badge Holder,...            1   \n",
       "5629  Google Men's 100% Cotton Short Sleeve Hero Tee...            3   \n",
       "\n",
       "      total_visits  total_hits  Price  total_timeOnSite  total_newVisits  \\\n",
       "0                1           9  16.99              89.0                1   \n",
       "1                1           7  99.99             421.0                0   \n",
       "2                1          53  11.99             755.0                1   \n",
       "3                1           8  16.99              82.0                1   \n",
       "4                1          33  16.99             465.0                0   \n",
       "...            ...         ...    ...               ...              ...   \n",
       "5625             1          38  16.99             494.0                1   \n",
       "5626             1          16  18.99             895.0                0   \n",
       "5627             1          13   1.99             392.0                1   \n",
       "5628             1          53  12.79            2430.0                1   \n",
       "5629             1          23  13.59             568.0                0   \n",
       "\n",
       "      total_pageviews  \n",
       "0                   7  \n",
       "1                   5  \n",
       "2                  36  \n",
       "3                   6  \n",
       "4                  20  \n",
       "...               ...  \n",
       "5625               27  \n",
       "5626               13  \n",
       "5627               12  \n",
       "5628               44  \n",
       "5629               18  \n",
       "\n",
       "[5381 rows x 11 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([data,buy_df], axis=0,ignore_index=True)\n",
    "df.drop_duplicates(subset='visitId',inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df  # New df with whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Didn't Sell    4599\n",
       "Sell            782\n",
       "Name: Sells, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = df.Sells.value_counts()\n",
    "inst.index = [\"Didn't Sell\",\"Sell\"]   # type: ignore\n",
    "inst # Now we see we have a better distribution of out target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since the distribution is not very well done in terms of balance between sells and not sold, we need to take that into account when watching how the model goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Didn't Sell    1500\n",
       "Sell            700\n",
       "Name: Sells, dtype: int64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# Filter the DataFrame to balance the target variable\n",
    "df_balanced = df.groupby('Sells').apply(lambda x: x.sample(n=700) if x['Sells'].values[0] == 1 else x.sample(n=1500))\n",
    "df_balanced = df_balanced.droplevel(0)  # Remove the grouping level\n",
    "\n",
    "# Create a list of all row indexes\n",
    "all_indexes = list(df_balanced.index)\n",
    "\n",
    "inst = df_balanced.Sells.value_counts()\n",
    "inst.index = [\"Didn't Sell\",\"Sell\"]   # type: ignore\n",
    "inst # Now we see we have a better distribution of out target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the proportion of data to include in the training set\n",
    "train_proportion = 0.8  # 80% for training, 20% for testing\n",
    "\n",
    "# Calculate the number of rows for training and testing\n",
    "num_rows = len(df_balanced)\n",
    "num_train = int(train_proportion * num_rows)\n",
    "num_test = num_rows - num_train\n",
    "\n",
    "# Randomly sample indexes for the training set\n",
    "train_indexes = random.sample(all_indexes, num_train)\n",
    "\n",
    "# Remove the selected training indexes to get testing indexes\n",
    "test_indexes = list(set(all_indexes) - set(train_indexes))\n",
    "\n",
    "# Create the training and testing DataFrames using the sampled indexes\n",
    "train_df = df_balanced.loc[train_indexes]\n",
    "test_df = df_balanced.loc[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Scoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up a simple Model that contains Linear Layers for beahvior features, another branch for processing the embeddings, and a Sigmoid head. \n",
    "\n",
    "First, we inspect the posibility of getting it done with some backend suppot of our GPU,which can greatly augment the velocity of computation and tehrefore trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend available\n"
     ]
    }
   ],
   "source": [
    "# check availability of gpu in you system \n",
    "import torch \n",
    "\n",
    "if torch.cuda.is_available() and torch.backends.cuda.is_built(): device='cuda'; print(\"Cuda Available\") \n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built(): device='mps'; print('MPS backend available')\n",
    "else: device = 'cpu'; print('CPU is going to be used') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate and ttest with some toy data observing that it does give us an output containing (batch_size,1) were each element is the probability of that visit Session, ending in a Sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4613],\n",
      "        [0.5280],\n",
      "        [0.5231],\n",
      "        [0.5409],\n",
      "        [0.5728],\n",
      "        [0.5265],\n",
      "        [0.5236],\n",
      "        [0.5076],\n",
      "        [0.5533],\n",
      "        [0.5267],\n",
      "        [0.5178],\n",
      "        [0.4965],\n",
      "        [0.4675],\n",
      "        [0.4831],\n",
      "        [0.5437],\n",
      "        [0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      " torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_in_channels,\n",
    "        num_behavioural_features,\n",
    "        conv_out_channels,\n",
    "        conv_kernel_size=3,\n",
    "        stride=3,\n",
    "        pool_size=2,\n",
    "    ):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Linear layers for behavioral features\n",
    "        self.behavioural_layers = nn.Sequential(\n",
    "            nn.Linear(num_behavioural_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Convolutional layers for embeddings\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                conv_in_channels,\n",
    "                conv_out_channels,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=32),\n",
    "        )\n",
    "\n",
    "        # Linear layer for combining features\n",
    "\n",
    "        self.output_size = int(\n",
    "            conv_out_channels\n",
    "            * np.floor(((768 - conv_kernel_size) / stride) + 1)\n",
    "            / pool_size\n",
    "        )\n",
    "        self.final_linear = nn.Linear(16 + self.output_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, behavioural_features, embeddings):\n",
    "        behavioural_output = self.behavioural_layers(behavioural_features)\n",
    "        # embeddings = embeddings.unsqueeze(2)  # Add a channel dimension for conv1d\n",
    "        conv_output = self.conv_layers(embeddings)\n",
    "        conv_output = torch.flatten(conv_output, start_dim=1)  # Flatten conv output\n",
    "\n",
    "        combined_features = torch.cat((behavioural_output, conv_output), dim=1)\n",
    "        final_linear = self.final_linear(combined_features)\n",
    "        final_output = self.sigmoid(final_linear)  # Get probabilities\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def train_model(\n",
    "            self,\n",
    "            train_loader,\n",
    "            num_epochs=5,\n",
    "            learning_rate=1e-3,\n",
    "            validation_loader=None,\n",
    "            device=None,\n",
    "        ):\n",
    "        if device is None:\n",
    "            device = torch.device(\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "            )\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            self.to(device)\n",
    "\n",
    "        criterion = nn.BCELoss()                                           # Binary Cross Entropy\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)  # Adam Optimizer\n",
    "        \n",
    "        loss_hist = {'train':[],\n",
    "                     'val':[]}\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            data_generator = train_loader\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            for i, (behavioural_features, embeddings, labels) in enumerate(\n",
    "                data_generator, start=1\n",
    "            ):\n",
    "                behavioural_features = behavioural_features.to(device)\n",
    "                embeddings = embeddings.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(behavioural_features, embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                progress_percentage = i / len(data_generator)\n",
    "                progress_bar_length = 100\n",
    "                num_chars = int(progress_percentage * progress_bar_length)\n",
    "                progress_bar = \"[\" + \"-\" * num_chars + \" \" * (progress_bar_length - num_chars) + \"]\"\n",
    "                \n",
    "                # Clear the previous console message and update with current loss\n",
    "                print(\n",
    "                f\"\\r\\033[1mBatch {i}/{len(data_generator)} - Loss: {total_loss / i:.4f}\\033[0m {progress_bar} {progress_percentage * 100:.2f}%\",\n",
    "                end=\"\",\n",
    "            )\n",
    "            \n",
    "            loss_hist['train'].append(total_loss) # Append loss to history \n",
    "\n",
    "            print(\"\")  # Move to a new line after completing the epoch\n",
    "\n",
    "            # Validation loop\n",
    "\n",
    "            if validation_loader:\n",
    "\n",
    "                val_loader = validation_loader\n",
    "\n",
    "                self.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for behavioural_features, embeddings, labels in val_loader:\n",
    "                        behavioural_features = behavioural_features.to(device)\n",
    "                        embeddings = embeddings.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        val_outputs = self(behavioural_features, embeddings)\n",
    "                        val_loss += criterion(val_outputs, labels).item()\n",
    "\n",
    "                val_loss /= len(val_loader)\n",
    "                print(\n",
    "                    f\"Validation Loss (Epoch {epoch + 1}/{num_epochs}): {val_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "                loss_hist['val'].append(val_loss) # Append loss to history \n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "embedding_dim = 768\n",
    "num_behavioural_features = 10  #  number of behavioural features\n",
    "num_object_embedded = 5  #  number of in channels to apply conv1d to\n",
    "conv_out_channels = 16\n",
    "conv_kernel_size = 3\n",
    "\n",
    "model = CombinedModel(\n",
    "    num_object_embedded,\n",
    "    num_behavioural_features,\n",
    "    conv_out_channels,\n",
    "    conv_kernel_size,\n",
    "    pool_size=32,\n",
    ")\n",
    "\n",
    "# Generate some sample inputs\n",
    "behavioural_features = torch.randn(16, num_behavioural_features)  # Batch size 16\n",
    "embeddings = torch.randn(16, num_object_embedded, embedding_dim)\n",
    "\n",
    "# Pass inputs through the model\n",
    "output = model(behavioural_features, embeddings)\n",
    "print(output)\n",
    "print(\"\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Training the Score Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim Or Pad Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_or_pad(seq:torch.Tensor,max_length=5):\n",
    "    if seq.shape[1] < max_length:\n",
    "        # If sequence length is less than max_length, pad with zeros\n",
    "        pad_size = max_length - seq.shape[1]\n",
    "        padding = torch.zeros(seq.shape[0], pad_size, seq.shape[2])\n",
    "        return torch.cat((seq, padding), dim=1)\n",
    "    else:\n",
    "        # If sequence length is greater than max_length, randomly select max_length elements\n",
    "        indices = np.random.choice(range(max_length),max_length)\n",
    "        return seq[:, indices, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a vector whose examples are more than 5 we trim leaving the size from [16, 13, 500] --> [16, 5, 500]\n",
      "On the other hand, for a vector whose examples are less than 5 we pad leaving the size from [16, 2, 500] --> [16, 5, 500]\n"
     ]
    }
   ],
   "source": [
    "print(f\"For a vector whose examples are more than 5 we trim leaving the size from {list(torch.randn(16, 13, 500).shape)} --> {list(trim_or_pad(torch.randn(16, 13, 500)).shape)}\")\n",
    "print(f\"On the other hand, for a vector whose examples are less than 5 we pad leaving the size from {list(torch.randn(16, 2, 500).shape)} --> {list(trim_or_pad(torch.randn(16, 2, 500)).shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataGenerator:\n",
    "      \n",
    "    def __init__(self, data, labels, batch_size=64, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(labels.shape[0])\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_idx >= self.labels.shape[0]:\n",
    "            raise StopIteration\n",
    "\n",
    "        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]\n",
    "\n",
    "        features_batch = self.data['features'][batch_indices]\n",
    "        embeddings_batch = self.data['embeddings'][batch_indices]\n",
    "        labels_batch = self.labels[batch_indices]\n",
    "\n",
    "        self.current_idx += self.batch_size\n",
    "\n",
    "        return features_batch,embeddings_batch, labels_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0] // self.batch_size\n",
    "\n",
    "drop_features_names = ['visitId','Product']\n",
    "\n",
    "train = train_df.drop(drop_features_names,axis=1).reset_index(drop=True)\n",
    "test = test_df.drop(drop_features_names,axis=1).reset_index(drop=True)\n",
    "\n",
    "train_embeddings = train.Embeddings.apply(lambda x : trim_or_pad(\n",
    "    torch.tensor(x ,dtype=torch.float32).permute(1,0,2).contiguous())) # type: ignore\n",
    "\n",
    "test_embeddings  = test.Embeddings.apply(lambda x: trim_or_pad(\n",
    "    torch.tensor(x,dtype=torch.float32).permute(1,0,2).contiguous())) # type: ignore\n",
    "\n",
    "train_labels = torch.tensor(train.Sells.values,dtype=torch.float32).view(-1,1)\n",
    "test_labels = torch.tensor(test.Sells.values,dtype=torch.float32).view(-1,1)\n",
    "\n",
    "train = {'features': torch.tensor(train.drop(['Sells', 'Embeddings'], axis=1).values, dtype=torch.float32),\n",
    "         'embeddings': torch.concat(train_embeddings.to_list(),dim=0)}\n",
    "\n",
    "test = {'features': torch.tensor(test.drop(['Sells', 'Embeddings'], axis=1).values, dtype=torch.float32),\n",
    "         'embeddings': torch.concat(test_embeddings.to_list(),dim=0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings have a shape of --> [32, 5, 768]\n",
      "The features have a shape of -->[32, 7]\n",
      "The true value labels have a shape of --> [32, 1]\n",
      "The model's output has a shape --> [32, 1]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "train_generator = CustomDataGenerator(data=train, labels=train_labels, batch_size=32, shuffle=True)\n",
    "test_generator = CustomDataGenerator(data=test, labels=test_labels, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example of iterating through the data generator\n",
    "for data_batch in train_generator:\n",
    "    features, embeddings , labels  = data_batch\n",
    "    print(f\"The embeddings have a shape of --> {list(embeddings.shape)}\")\n",
    "    print(f\"The features have a shape of -->{list(features.shape)}\")\n",
    "    print(f\"The true value labels have a shape of --> {list(labels.shape)}\")\n",
    "    break \n",
    "        \n",
    "# Instantiate the model\n",
    "embedding_dim = embeddings.shape[2]\n",
    "num_behavioural_features = features.shape[1]  #  number of behavioural features\n",
    "num_object_embedded = embeddings.shape[1]        #  number of in channels to apply conv1d to \n",
    "conv_out_channels = 16\n",
    "conv_kernel_size = 3\n",
    "\n",
    "model = CombinedModel(num_object_embedded, num_behavioural_features, conv_out_channels, conv_kernel_size,pool_size=32)\n",
    "\n",
    "print(f\"The model's output has a shape --> {list(model(features,embeddings).shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that everything is apparently ok, we go and train for a few epochs, which will proceed to the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.6346\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 1/300): 0.6109\n",
      "Epoch 2/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.5307\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 2/300): 0.5930\n",
      "Epoch 3/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.5089\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 3/300): 0.5881\n",
      "Epoch 4/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4770\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 4/300): 0.5646\n",
      "Epoch 5/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4684\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 5/300): 0.6302\n",
      "Epoch 6/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4660\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 6/300): 0.5487\n",
      "Epoch 7/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4398\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 7/300): 0.5502\n",
      "Epoch 8/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4339\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 8/300): 0.5337\n",
      "Epoch 9/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4271\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 9/300): 0.5372\n",
      "Epoch 10/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4270\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 10/300): 0.5367\n",
      "Epoch 11/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4179\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 11/300): 0.5376\n",
      "Epoch 12/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4178\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 12/300): 0.5362\n",
      "Epoch 13/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4255\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 13/300): 0.5350\n",
      "Epoch 14/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4151\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 14/300): 0.5853\n",
      "Epoch 15/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4162\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 15/300): 0.5217\n",
      "Epoch 16/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4085\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 16/300): 0.5378\n",
      "Epoch 17/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4061\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 17/300): 0.5202\n",
      "Epoch 18/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4016\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 18/300): 0.5402\n",
      "Epoch 19/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4109\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 19/300): 0.5175\n",
      "Epoch 20/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3993\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 20/300): 0.5150\n",
      "Epoch 21/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3942\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 21/300): 0.5343\n",
      "Epoch 22/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.4029\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 22/300): 0.5199\n",
      "Epoch 23/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3949\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 23/300): 0.5215\n",
      "Epoch 24/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3957\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 24/300): 0.5239\n",
      "Epoch 25/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3928\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 25/300): 0.5240\n",
      "Epoch 26/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3883\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 26/300): 0.5104\n",
      "Epoch 27/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3890\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 27/300): 0.5138\n",
      "Epoch 28/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3862\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 28/300): 0.4972\n",
      "Epoch 29/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3840\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 29/300): 0.5174\n",
      "Epoch 30/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3855\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 30/300): 0.5085\n",
      "Epoch 31/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3952\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 31/300): 0.5188\n",
      "Epoch 32/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3824\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 32/300): 0.5198\n",
      "Epoch 33/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3826\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 33/300): 0.5114\n",
      "Epoch 34/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3896\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 34/300): 0.5152\n",
      "Epoch 35/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3768\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 35/300): 0.5008\n",
      "Epoch 36/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3773\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 36/300): 0.4985\n",
      "Epoch 37/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3760\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 37/300): 0.5039\n",
      "Epoch 38/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3779\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 38/300): 0.5005\n",
      "Epoch 39/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3736\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 39/300): 0.4931\n",
      "Epoch 40/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3737\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 40/300): 0.4976\n",
      "Epoch 41/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3748\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 41/300): 0.5009\n",
      "Epoch 42/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3774\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 42/300): 0.5089\n",
      "Epoch 43/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3728\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 43/300): 0.4993\n",
      "Epoch 44/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3660\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 44/300): 0.5113\n",
      "Epoch 45/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3750\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 45/300): 0.4915\n",
      "Epoch 46/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3729\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 46/300): 0.4984\n",
      "Epoch 47/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3669\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 47/300): 0.4935\n",
      "Epoch 48/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3558\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 48/300): 0.4866\n",
      "Epoch 49/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3610\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 49/300): 0.4842\n",
      "Epoch 50/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3570\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 50/300): 0.4861\n",
      "Epoch 51/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3643\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 51/300): 0.5038\n",
      "Epoch 52/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3566\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 52/300): 0.4762\n",
      "Epoch 53/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3669\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 53/300): 0.4826\n",
      "Epoch 54/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3686\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 54/300): 0.4701\n",
      "Epoch 55/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3628\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 55/300): 0.4807\n",
      "Epoch 56/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3612\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 56/300): 0.4699\n",
      "Epoch 57/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3591\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 57/300): 0.4719\n",
      "Epoch 58/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3503\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 58/300): 0.4773\n",
      "Epoch 59/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3653\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 59/300): 0.4766\n",
      "Epoch 60/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3589\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 60/300): 0.4719\n",
      "Epoch 61/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3527\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 61/300): 0.4693\n",
      "Epoch 62/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3497\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 62/300): 0.4706\n",
      "Epoch 63/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3489\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 63/300): 0.4600\n",
      "Epoch 64/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3660\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 64/300): 0.4721\n",
      "Epoch 65/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3606\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 65/300): 0.4692\n",
      "Epoch 66/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3696\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 66/300): 0.4736\n",
      "Epoch 67/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3522\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 67/300): 0.4663\n",
      "Epoch 68/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3451\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 68/300): 0.4777\n",
      "Epoch 69/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3536\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 69/300): 0.4640\n",
      "Epoch 70/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3481\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 70/300): 0.4579\n",
      "Epoch 71/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3484\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 71/300): 0.4827\n",
      "Epoch 72/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3671\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 72/300): 0.4845\n",
      "Epoch 73/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3453\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 73/300): 0.4691\n",
      "Epoch 74/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3420\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 74/300): 0.4666\n",
      "Epoch 75/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3453\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 75/300): 0.4610\n",
      "Epoch 76/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3385\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 76/300): 0.4681\n",
      "Epoch 77/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3411\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 77/300): 0.4662\n",
      "Epoch 78/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3550\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 78/300): 0.4786\n",
      "Epoch 79/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3369\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 79/300): 0.4699\n",
      "Epoch 80/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3430\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 80/300): 0.4769\n",
      "Epoch 81/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3392\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 81/300): 0.4728\n",
      "Epoch 82/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3390\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 82/300): 0.4655\n",
      "Epoch 83/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3397\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 83/300): 0.4628\n",
      "Epoch 84/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3408\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 84/300): 0.4682\n",
      "Epoch 85/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3366\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 85/300): 0.4670\n",
      "Epoch 86/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3372\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 86/300): 0.4620\n",
      "Epoch 87/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3397\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 87/300): 0.4711\n",
      "Epoch 88/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3358\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 88/300): 0.4589\n",
      "Epoch 89/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3434\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 89/300): 0.4699\n",
      "Epoch 90/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3371\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 90/300): 0.4781\n",
      "Epoch 91/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3355\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 91/300): 0.4710\n",
      "Epoch 92/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3331\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 92/300): 0.4769\n",
      "Epoch 93/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3291\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 93/300): 0.4628\n",
      "Epoch 94/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3347\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 94/300): 0.4793\n",
      "Epoch 95/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3301\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 95/300): 0.4683\n",
      "Epoch 96/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3375\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 96/300): 0.4743\n",
      "Epoch 97/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3300\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 97/300): 0.4680\n",
      "Epoch 98/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3284\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 98/300): 0.4605\n",
      "Epoch 99/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3366\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 99/300): 0.4743\n",
      "Epoch 100/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3341\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 100/300): 0.4747\n",
      "Epoch 101/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3315\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 101/300): 0.4696\n",
      "Epoch 102/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3260\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 102/300): 0.4598\n",
      "Epoch 103/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3275\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 103/300): 0.4617\n",
      "Epoch 104/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3309\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 104/300): 0.4634\n",
      "Epoch 105/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3373\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 105/300): 0.4692\n",
      "Epoch 106/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3377\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 106/300): 0.4604\n",
      "Epoch 107/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3350\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 107/300): 0.4750\n",
      "Epoch 108/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3309\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 108/300): 0.4657\n",
      "Epoch 109/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3294\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 109/300): 0.4616\n",
      "Epoch 110/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3245\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 110/300): 0.4579\n",
      "Epoch 111/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3235\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 111/300): 0.4574\n",
      "Epoch 112/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3259\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 112/300): 0.4586\n",
      "Epoch 113/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3260\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 113/300): 0.4526\n",
      "Epoch 114/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3326\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 114/300): 0.4639\n",
      "Epoch 115/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3271\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 115/300): 0.4616\n",
      "Epoch 116/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3273\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 116/300): 0.4649\n",
      "Epoch 117/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3277\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 117/300): 0.4658\n",
      "Epoch 118/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3248\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 118/300): 0.4615\n",
      "Epoch 119/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3202\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 119/300): 0.4477\n",
      "Epoch 120/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3337\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 120/300): 0.4576\n",
      "Epoch 121/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3268\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 121/300): 0.4704\n",
      "Epoch 122/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3261\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 122/300): 0.4718\n",
      "Epoch 123/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3272\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 123/300): 0.4664\n",
      "Epoch 124/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3207\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 124/300): 0.4569\n",
      "Epoch 125/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3191\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 125/300): 0.4538\n",
      "Epoch 126/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3213\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 126/300): 0.4452\n",
      "Epoch 127/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3200\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 127/300): 0.4436\n",
      "Epoch 128/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3258\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 128/300): 0.4538\n",
      "Epoch 129/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3267\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 129/300): 0.4608\n",
      "Epoch 130/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3204\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 130/300): 0.4515\n",
      "Epoch 131/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3281\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 131/300): 0.4536\n",
      "Epoch 132/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3301\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 132/300): 0.4476\n",
      "Epoch 133/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3279\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 133/300): 0.4636\n",
      "Epoch 134/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3203\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 134/300): 0.4564\n",
      "Epoch 135/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3189\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 135/300): 0.4632\n",
      "Epoch 136/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3201\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 136/300): 0.4589\n",
      "Epoch 137/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3187\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 137/300): 0.4495\n",
      "Epoch 138/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3198\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 138/300): 0.4520\n",
      "Epoch 139/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3173\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 139/300): 0.4482\n",
      "Epoch 140/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3194\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 140/300): 0.4562\n",
      "Epoch 141/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3224\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 141/300): 0.4564\n",
      "Epoch 142/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3182\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 142/300): 0.4592\n",
      "Epoch 143/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3134\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 143/300): 0.4513\n",
      "Epoch 144/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3155\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 144/300): 0.4560\n",
      "Epoch 145/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3138\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 145/300): 0.4489\n",
      "Epoch 146/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3154\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 146/300): 0.4537\n",
      "Epoch 147/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3137\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 147/300): 0.4389\n",
      "Epoch 148/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3186\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 148/300): 0.4369\n",
      "Epoch 149/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3150\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 149/300): 0.4435\n",
      "Epoch 150/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3140\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 150/300): 0.4547\n",
      "Epoch 151/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3144\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 151/300): 0.4520\n",
      "Epoch 152/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3169\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 152/300): 0.4523\n",
      "Epoch 153/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3160\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 153/300): 0.4413\n",
      "Epoch 154/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3156\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 154/300): 0.4516\n",
      "Epoch 155/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3100\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 155/300): 0.4408\n",
      "Epoch 156/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3120\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 156/300): 0.4411\n",
      "Epoch 157/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3121\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 157/300): 0.4455\n",
      "Epoch 158/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3124\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 158/300): 0.4585\n",
      "Epoch 159/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3097\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 159/300): 0.4441\n",
      "Epoch 160/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3093\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 160/300): 0.4319\n",
      "Epoch 161/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3123\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 161/300): 0.4457\n",
      "Epoch 162/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3080\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 162/300): 0.4715\n",
      "Epoch 163/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3125\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 163/300): 0.4511\n",
      "Epoch 164/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3107\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 164/300): 0.4396\n",
      "Epoch 165/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3110\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 165/300): 0.4378\n",
      "Epoch 166/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3083\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 166/300): 0.4560\n",
      "Epoch 167/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3084\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 167/300): 0.4411\n",
      "Epoch 168/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3098\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 168/300): 0.4492\n",
      "Epoch 169/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3118\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 169/300): 0.4415\n",
      "Epoch 170/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3074\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 170/300): 0.4370\n",
      "Epoch 171/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3042\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 171/300): 0.4411\n",
      "Epoch 172/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3063\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 172/300): 0.4449\n",
      "Epoch 173/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3050\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 173/300): 0.4526\n",
      "Epoch 174/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3055\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 174/300): 0.4449\n",
      "Epoch 175/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3057\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 175/300): 0.4599\n",
      "Epoch 176/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3049\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 176/300): 0.4454\n",
      "Epoch 177/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3031\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 177/300): 0.4467\n",
      "Epoch 178/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3032\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 178/300): 0.4559\n",
      "Epoch 179/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3062\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 179/300): 0.4358\n",
      "Epoch 180/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3062\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 180/300): 0.4469\n",
      "Epoch 181/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3032\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 181/300): 0.4494\n",
      "Epoch 182/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3030\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 182/300): 0.4268\n",
      "Epoch 183/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3049\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 183/300): 0.4556\n",
      "Epoch 184/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3009\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 184/300): 0.4522\n",
      "Epoch 185/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3002\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 185/300): 0.4525\n",
      "Epoch 186/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3006\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 186/300): 0.4508\n",
      "Epoch 187/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2981\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 187/300): 0.4406\n",
      "Epoch 188/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3034\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 188/300): 0.4705\n",
      "Epoch 189/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.3000\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 189/300): 0.4647\n",
      "Epoch 190/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2980\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 190/300): 0.4415\n",
      "Epoch 191/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2971\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 191/300): 0.4550\n",
      "Epoch 192/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2993\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 192/300): 0.4582\n",
      "Epoch 193/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2986\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 193/300): 0.4514\n",
      "Epoch 194/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2985\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 194/300): 0.4588\n",
      "Epoch 195/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2965\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 195/300): 0.4648\n",
      "Epoch 196/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2965\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 196/300): 0.4545\n",
      "Epoch 197/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2961\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 197/300): 0.4441\n",
      "Epoch 198/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2996\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 198/300): 0.4572\n",
      "Epoch 199/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2970\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 199/300): 0.4601\n",
      "Epoch 200/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2962\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 200/300): 0.4551\n",
      "Epoch 201/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2958\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 201/300): 0.4583\n",
      "Epoch 202/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2990\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 202/300): 0.4498\n",
      "Epoch 203/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2955\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 203/300): 0.4348\n",
      "Epoch 204/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2974\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 204/300): 0.4613\n",
      "Epoch 205/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2987\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 205/300): 0.4468\n",
      "Epoch 206/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2971\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 206/300): 0.4444\n",
      "Epoch 207/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2968\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 207/300): 0.4593\n",
      "Epoch 208/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2954\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 208/300): 0.4521\n",
      "Epoch 209/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2964\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 209/300): 0.4587\n",
      "Epoch 210/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2959\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 210/300): 0.4496\n",
      "Epoch 211/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2937\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 211/300): 0.4505\n",
      "Epoch 212/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2954\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 212/300): 0.4574\n",
      "Epoch 213/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2932\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 213/300): 0.4611\n",
      "Epoch 214/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2936\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 214/300): 0.4604\n",
      "Epoch 215/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2942\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 215/300): 0.4484\n",
      "Epoch 216/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2928\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 216/300): 0.4483\n",
      "Epoch 217/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2972\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 217/300): 0.4445\n",
      "Epoch 218/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2916\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 218/300): 0.4758\n",
      "Epoch 219/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2921\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 219/300): 0.4658\n",
      "Epoch 220/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2920\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 220/300): 0.4910\n",
      "Epoch 221/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2918\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 221/300): 0.4790\n",
      "Epoch 222/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2912\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 222/300): 0.4791\n",
      "Epoch 223/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2936\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 223/300): 0.4638\n",
      "Epoch 224/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2903\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 224/300): 0.4481\n",
      "Epoch 225/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2882\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 225/300): 0.4526\n",
      "Epoch 226/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2889\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 226/300): 0.4792\n",
      "Epoch 227/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2889\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 227/300): 0.4553\n",
      "Epoch 228/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2906\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 228/300): 0.4626\n",
      "Epoch 229/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2885\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 229/300): 0.4698\n",
      "Epoch 230/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2898\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 230/300): 0.4519\n",
      "Epoch 231/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2913\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 231/300): 0.4712\n",
      "Epoch 232/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2905\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 232/300): 0.4836\n",
      "Epoch 233/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2899\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 233/300): 0.4886\n",
      "Epoch 234/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2895\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 234/300): 0.4575\n",
      "Epoch 235/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2859\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 235/300): 0.4728\n",
      "Epoch 236/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2879\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 236/300): 0.4707\n",
      "Epoch 237/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2871\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 237/300): 0.4678\n",
      "Epoch 238/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2860\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 238/300): 0.4843\n",
      "Epoch 239/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2878\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 239/300): 0.4884\n",
      "Epoch 240/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2871\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 240/300): 0.4936\n",
      "Epoch 241/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2876\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 241/300): 0.4710\n",
      "Epoch 242/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2866\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 242/300): 0.4955\n",
      "Epoch 243/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2851\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 243/300): 0.4911\n",
      "Epoch 244/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2844\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 244/300): 0.4862\n",
      "Epoch 245/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2870\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 245/300): 0.4812\n",
      "Epoch 246/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2858\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 246/300): 0.4702\n",
      "Epoch 247/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2840\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 247/300): 0.4813\n",
      "Epoch 248/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2827\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 248/300): 0.4856\n",
      "Epoch 249/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2851\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 249/300): 0.4708\n",
      "Epoch 250/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2827\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 250/300): 0.4669\n",
      "Epoch 251/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2826\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 251/300): 0.4628\n",
      "Epoch 252/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2811\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 252/300): 0.4609\n",
      "Epoch 253/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2832\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 253/300): 0.4825\n",
      "Epoch 254/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2829\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 254/300): 0.5166\n",
      "Epoch 255/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2830\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 255/300): 0.5089\n",
      "Epoch 256/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2839\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 256/300): 0.4819\n",
      "Epoch 257/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2811\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 257/300): 0.4834\n",
      "Epoch 258/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2815\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 258/300): 0.4877\n",
      "Epoch 259/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2819\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 259/300): 0.4630\n",
      "Epoch 260/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2793\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 260/300): 0.4835\n",
      "Epoch 261/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2799\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 261/300): 0.4982\n",
      "Epoch 262/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2809\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 262/300): 0.4860\n",
      "Epoch 263/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2801\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 263/300): 0.5021\n",
      "Epoch 264/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2804\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 264/300): 0.4833\n",
      "Epoch 265/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2783\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 265/300): 0.4993\n",
      "Epoch 266/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2797\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 266/300): 0.5248\n",
      "Epoch 267/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2787\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 267/300): 0.4907\n",
      "Epoch 268/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2767\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 268/300): 0.4638\n",
      "Epoch 269/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2776\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 269/300): 0.4859\n",
      "Epoch 270/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2765\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 270/300): 0.4867\n",
      "Epoch 271/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2766\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 271/300): 0.4894\n",
      "Epoch 272/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2756\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 272/300): 0.5156\n",
      "Epoch 273/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2782\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 273/300): 0.4948\n",
      "Epoch 274/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2765\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 274/300): 0.5003\n",
      "Epoch 275/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2758\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 275/300): 0.5044\n",
      "Epoch 276/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2748\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 276/300): 0.4945\n",
      "Epoch 277/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2769\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 277/300): 0.4845\n",
      "Epoch 278/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2753\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 278/300): 0.5072\n",
      "Epoch 279/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2739\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 279/300): 0.5043\n",
      "Epoch 280/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2755\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 280/300): 0.5015\n",
      "Epoch 281/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2752\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 281/300): 0.5202\n",
      "Epoch 282/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2740\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 282/300): 0.5120\n",
      "Epoch 283/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2773\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 283/300): 0.4687\n",
      "Epoch 284/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2844\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 284/300): 0.4643\n",
      "Epoch 285/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2780\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 285/300): 0.4521\n",
      "Epoch 286/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2763\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 286/300): 0.4633\n",
      "Epoch 287/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2899\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 287/300): 0.4495\n",
      "Epoch 288/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2817\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 288/300): 0.4689\n",
      "Epoch 289/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2802\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 289/300): 0.4848\n",
      "Epoch 290/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2794\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 290/300): 0.4752\n",
      "Epoch 291/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2750\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 291/300): 0.4613\n",
      "Epoch 292/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2758\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 292/300): 0.4700\n",
      "Epoch 293/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2756\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 293/300): 0.4875\n",
      "Epoch 294/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2741\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 294/300): 0.4766\n",
      "Epoch 295/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2761\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 295/300): 0.4919\n",
      "Epoch 296/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2789\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 296/300): 0.4661\n",
      "Epoch 297/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2762\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 297/300): 0.4780\n",
      "Epoch 298/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2747\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 298/300): 0.4771\n",
      "Epoch 299/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2730\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 299/300): 0.4994\n",
      "Epoch 300/300\n",
      "\u001b[1mBatch 55/55 - Loss: 0.2759\u001b[0m [----------------------------------------------------------------------------------------------------] 100.00%\n",
      "Validation Loss (Epoch 300/300): 0.4812\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model.train_model(train_generator, num_epochs=300,\n",
    "            learning_rate=1e-3,\n",
    "            validation_loader=test_generator,\n",
    "            device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8113636363636364 \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.92      0.87       291\n",
      "         1.0       0.79      0.60      0.68       149\n",
      "\n",
      "    accuracy                           0.81       440\n",
      "   macro avg       0.80      0.76      0.77       440\n",
      "weighted avg       0.81      0.81      0.80       440\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "[[267  24]\n",
      " [ 59  90]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features,embeddings,labels in test_generator:\n",
    "        outputs = model(features,embeddings)\n",
    "        predicted_class = np.round(outputs.detach().numpy())\n",
    "        predictions.extend(predicted_class)\n",
    "        ground_truth.extend(labels.detach().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true=ground_truth,y_pred=predictions)}\",\"\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_true=ground_truth,y_pred=predictions)}\",\"\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_true=ground_truth,y_pred=predictions)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
